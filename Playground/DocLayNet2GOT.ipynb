{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "train_json = \"/home/t-jiaweiwang/Project/GOT-OCR2.0/datasets/DocLayNet/train_textlines_others_add_text_400_converted_add_in_graphical_others_grouped.json\"\n",
    "\n",
    "train_annotations = json.load(open(train_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 1, 'name': 'Picture', 'supercategory': 'super fig'},\n",
       " {'id': 2, 'name': 'Table', 'supercategory': 'super tab'},\n",
       " {'id': 3, 'name': 'Formula', 'supercategory': 'super formula'},\n",
       " {'id': 4, 'name': 'Paragraph', 'supercategory': 'super para'},\n",
       " {'id': 5, 'name': 'other', 'supercategory': 'super other'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_annotations['categories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nConversation Format:\\n\\n{\\n    \"image\": \"image_path\",\\n    \"conversations\": [\\n        {\\n            \"from\": \"human\",\\n            \"value\": \"<image>\\nPOD: \"\\n        },\\n        {\\n            \"from\": \"gpt\",\\n            \"value\": \"[Box1] [Logical Role]\\n[Box2] [Logical Role]\\n...\"\\n        }\\n    ]\\n}\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Conversation Format:\n",
    "\n",
    "{\n",
    "    \"image\": \"image_path\",\n",
    "    \"conversations\": [\n",
    "        {\n",
    "            \"from\": \"human\",\n",
    "            \"value\": \"<image>\\nPOD: \"\n",
    "        },\n",
    "        {\n",
    "            \"from\": \"gpt\",\n",
    "            \"value\": \"[Box1] [Logical Role]\\n[Box2] [Logical Role]\\n...\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "'''\n",
    "\n",
    "conversations = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Formula', 'Table', 'Page-footer', 'others', 'Text', 'Picture', 'Footnote', 'Title', 'Page-header', 'List-item', 'Section-header', 'Caption'}\n"
     ]
    }
   ],
   "source": [
    "tags = set([anno['tags'][0] for anno in train_annotations['annotations']])\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageid2annotations = {}\n",
    "\n",
    "for annotation in train_annotations[\"annotations\"]:\n",
    "    if annotation[\"image_id\"] not in imageid2annotations:\n",
    "        imageid2annotations[annotation[\"image_id\"]] = []\n",
    "    imageid2annotations[annotation[\"image_id\"]].append(annotation)\n",
    "\n",
    "imageid2image = {}\n",
    "for image in train_annotations[\"images\"]:\n",
    "    imageid2image[image[\"id\"]] = image\n",
    "\n",
    "conversations = []\n",
    "for image_id, annotations in imageid2annotations.items():\n",
    "    image = imageid2image[image_id]\n",
    "    image_path = image[\"file_name\"]\n",
    "    conversation = {\n",
    "        \"image\": \"train/\" + image_path,\n",
    "        \"conversations\": []\n",
    "    }\n",
    "    conversation[\"conversations\"].append({\n",
    "        \"from\": \"human\",\n",
    "        \"value\": \"<image>\\nPOD: \"\n",
    "    })\n",
    "    gt_text = \"\"\n",
    "    gt_boxes = []\n",
    "    gt_logical_roles = []\n",
    "    for annotation in annotations:\n",
    "        bbox = annotation[\"quad_coord\"][:2] + annotation[\"quad_coord\"][4:6]\n",
    "        # x = int(x/w * 1000)\n",
    "        # y = int(y/h * 1000)\n",
    "        bbox = [int(bbox[0] / image[\"width\"] * 1000), int(bbox[1] / image[\"height\"] * 1000), int(bbox[2] / image[\"width\"] * 1000), int(bbox[3] / image[\"height\"] * 1000)]\n",
    "        if \"others\" in annotation[\"tags\"]:\n",
    "            # exclude others\n",
    "            continue\n",
    "        gt_boxes.append(bbox)\n",
    "        gt_logical_roles.append(annotation[\"tags\"][0])\n",
    "\n",
    "    # sort the boxes from top to bottom from left to right\n",
    "    # 将 gt_boxes 和 gt_logical_roles 结合在一起，根据 gt_boxes 进行排序\n",
    "    sorted_pairs = sorted(zip(gt_boxes, gt_logical_roles), key=lambda x: (x[0][1], x[0][0]))\n",
    "    if len(sorted_pairs) == 0:\n",
    "        continue\n",
    "    # 解压缩，提取排序后的 gt_boxes 和 gt_logical_roles\n",
    "    gt_boxes, gt_logical_roles = zip(*sorted_pairs)\n",
    "\n",
    "    # 将它们转换回列表（因为 zip 返回的是元组）\n",
    "    gt_boxes = list(gt_boxes)\n",
    "    gt_logical_roles = list(gt_logical_roles)\n",
    "    \n",
    "    for bbox, logical_role in zip(gt_boxes, gt_logical_roles):\n",
    "        gt_text += f\"{str(bbox)} {logical_role}\\n\"\n",
    "    \n",
    "    conversation[\"conversations\"].append({\n",
    "        \"from\": \"gpt\",\n",
    "        \"value\": gt_text\n",
    "    })\n",
    "    conversations.append(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conversations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mconversations\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'conversations' is not defined"
     ]
    }
   ],
   "source": [
    "conversations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(conversations, open(\"/home/t-jiaweiwang/Project/GOT-OCR2.0/datasets/DocLayNet/got_annotations/train_got_conversations_wo_others.json\", \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[70, 54, 433, 74] Page-header\n",
      "[97, 114, 867, 155] Text\n",
      "[97, 162, 866, 217] Text\n",
      "[97, 223, 861, 278] Text\n",
      "[97, 285, 813, 326] Text\n",
      "[97, 333, 836, 360] Text\n",
      "[94, 361, 891, 473] Table\n",
      "[97, 488, 876, 529] Text\n",
      "[97, 549, 789, 567] Section-header\n",
      "[97, 572, 608, 587] Text\n",
      "[96, 592, 886, 675] Table\n",
      "[97, 687, 886, 742] Text\n",
      "[97, 749, 782, 776] Text\n",
      "[97, 783, 887, 838] Text\n",
      "[924, 963, 947, 979] Page-footer\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可视化GT\n",
    "import json\n",
    "\n",
    "conversations = json.load(open(\"/home/t-jiaweiwang/Project/GOT-OCR2.0/datasets/DocLayNet/got_annotations/train_got_conversations_wo_others.json\"))\n",
    "converstation = conversations[0]\n",
    "image_path = \"/home/t-jiaweiwang/Project/GOT-OCR2.0/datasets/DocLayNet/\" + converstation[\"image\"]\n",
    "outputs = converstation[\"conversations\"][1][\"value\"]\n",
    "print(outputs)\n",
    "outputs_list = outputs.split('\\n')\n",
    "# visualize the bboxes and logical role in args.image_file: [117, 88, 328, 100] ['Section-header']\n",
    "import cv2\n",
    "image_viz = cv2.imread(image_path)\n",
    "w, h = image_viz.shape[1], image_viz.shape[0]\n",
    "for out in outputs_list:\n",
    "    try:\n",
    "        bbox, logical_role = out.split('] ')\n",
    "    except:\n",
    "        continue\n",
    "    bbox = bbox[1:].split(', ')\n",
    "    # bbox = [int(i) for i in bbox]\n",
    "    bbox = [int(i) / 1000 for i in bbox]\n",
    "    bbox = [int(bbox[0]*w), int(bbox[1]*h), int(bbox[2]*w), int(bbox[3]*h)]\n",
    "    if bbox[0] < 0 or bbox[1] < 0 or bbox[2] < 0 or bbox[3] < 0:\n",
    "        continue\n",
    "    image_viz = cv2.rectangle(image_viz, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 2)\n",
    "    image_viz = cv2.putText(image_viz, logical_role, (bbox[0], bbox[1]), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "cv2.imwrite('test_gt.jpg', image_viz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "val_data = json.load(open(\"/home/t-jiaweiwang/Project/GOT-OCR2.0/datasets/DocLayNet/val_textlines_others_add_text_new_coco.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "328279"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_data['annotations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data['annotations'] = [anno for anno in val_data['annotations'] if anno['category_id'] != 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99816"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data['annotations'].__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(val_data, open(\"/home/t-jiaweiwang/Project/GOT-OCR2.0/datasets/DocLayNet/val_textlines_others_add_text_new_coco_wo_others.json\", \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct data for localization and recognition concurrently\n",
    "\n",
    "import json\n",
    "\n",
    "train_json = \"/home/t-jiaweiwang/Project/GOT-OCR2.0/datasets/DocLayNet/train_textlines_others_add_text_400_converted_add_in_graphical_others_grouped.json\"\n",
    "\n",
    "train_annotations = json.load(open(train_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageid2annotations = {}\n",
    "\n",
    "for annotation in train_annotations[\"annotations\"]:\n",
    "    if annotation[\"image_id\"] not in imageid2annotations:\n",
    "        imageid2annotations[annotation[\"image_id\"]] = []\n",
    "    imageid2annotations[annotation[\"image_id\"]].append(annotation)\n",
    "\n",
    "imageid2image = {}\n",
    "for image in train_annotations[\"images\"]:\n",
    "    imageid2image[image[\"id\"]] = image\n",
    "\n",
    "conversations = []\n",
    "for image_id, annotations in imageid2annotations.items():\n",
    "    image = imageid2image[image_id]\n",
    "    image_path = image[\"file_name\"]\n",
    "    conversation = {\n",
    "        \"image\": \"train/\" + image_path,\n",
    "        \"conversations\": []\n",
    "    }\n",
    "    conversation[\"conversations\"].append({\n",
    "        \"from\": \"human\",\n",
    "        \"value\": \"<image>\\nPOD and OCR: \"\n",
    "    })\n",
    "    gt_text = \"\"\n",
    "    gt_boxes = []\n",
    "    gt_logical_roles = []\n",
    "    gt_texts = []\n",
    "    for annotation in annotations:\n",
    "        bbox = annotation[\"quad_coord\"][:2] + annotation[\"quad_coord\"][4:6]\n",
    "        # x = int(x/w * 1000)\n",
    "        # y = int(y/h * 1000)\n",
    "        bbox = [int(bbox[0] / image[\"width\"] * 1000), int(bbox[1] / image[\"height\"] * 1000), int(bbox[2] / image[\"width\"] * 1000), int(bbox[3] / image[\"height\"] * 1000)]\n",
    "        if \"others\" in annotation[\"tags\"]:\n",
    "            # exclude others\n",
    "            continue\n",
    "        gt_boxes.append(bbox)\n",
    "        gt_logical_roles.append(annotation[\"tags\"][0])\n",
    "        text_contents = ' '.join(annotation[\"textline_contents\"])\n",
    "        if text_contents:\n",
    "            gt_texts.append(text_contents)\n",
    "        else:\n",
    "            gt_texts.append(annotation[\"tags\"][0])\n",
    "\n",
    "    # sort the boxes from top to bottom from left to right\n",
    "    # 将 gt_boxes 和 gt_logical_roles 结合在一起，根据 gt_boxes 进行排序\n",
    "    sorted_pairs = sorted(zip(gt_boxes, gt_logical_roles, gt_texts), key=lambda x: (x[0][1], x[0][0]))\n",
    "    if len(sorted_pairs) == 0:\n",
    "        continue\n",
    "    # 解压缩，提取排序后的 gt_boxes 和 gt_logical_roles\n",
    "    gt_boxes, gt_logical_roles, gt_texts = zip(*sorted_pairs)\n",
    "\n",
    "    # 将它们转换回列表（因为 zip 返回的是元组）\n",
    "    gt_boxes = list(gt_boxes)\n",
    "    gt_logical_roles = list(gt_logical_roles)\n",
    "    gt_texts = list(gt_texts)\n",
    "    \n",
    "    for bbox, logical_role, text in zip(gt_boxes, gt_logical_roles, gt_texts):\n",
    "        gt_text += f\"{str(bbox)} {logical_role} <content>{text}</content>\\n\"\n",
    "    \n",
    "    conversation[\"conversations\"].append({\n",
    "        \"from\": \"gpt\",\n",
    "        \"value\": gt_text\n",
    "    })\n",
    "    conversations.append(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': 'train/c6effb847ae7e4a80431696984fa90c98bb08c266481b9a03842422459c43bdd.png',\n",
       " 'conversations': [{'from': 'human', 'value': '<image>\\nPOD and OCR: '},\n",
       "  {'from': 'gpt',\n",
       "   'value': '[70, 54, 433, 74] Page-header <content>NOTES TO THE FINANCIAL STATEMENTS</content>\\n[97, 114, 867, 155] Text <content>Finance receivables that originated outside the U.S. were $52.7 billion and $47.5 billion at December 31, 2004 and 2003, respectively. Other finance receivables consisted primarily of real estate, commercial and other collateralized loans and accrued interest. </content>\\n[97, 162, 866, 217] Text <content>Included in net finance and other receivables at December 31, 2004 and 2003 were $16.9 billion and $14.3 billion, respectively, of receivables that have been sold for legal purposes to consolidated securitization SPEs and are available only for repayment of debt issued by those entities, and to pay other securitization investors and other participants; they are not available to pay our other obligations or the claims of our other creditors.</content>\\n[97, 223, 861, 278] Text <content>Future maturities, exclusive of the effects of SFAS No. 133, Accounting for Derivative Instruments and Hedging Activities, of total finance receivables including minimum lease rentals are as follows (in billions): 2005 - $64.7; 2006 - $24.3;2007 - $13.9; thereafter - $10.1. Experience indicates that a substantial portion of the portfolio generally is repaid beforethe contractual maturity dates. </content>\\n[97, 285, 813, 326] Text <content>Finance receivables subject to fair value at December 31, 2004 and 2003 were $106.2 billion and $102.0 billion, respectively. The fair value of these finance receivables at December 31, 2004 and 2003 was $106.4 billion and$103.8 billion, respectively.</content>\\n[97, 333, 836, 360] Text <content>Included in retail receivables above are investments in direct financing leases. The net investment at December 31 was asfollows (in millions):</content>\\n[94, 361, 891, 473] Table <content>Table</content>\\n[97, 488, 876, 529] Text <content>The investment in direct financing leases relates to the leasing of vehicles, various types of transportation and other equipment and facilities. Future maturities of minimum lease rentals, as included above, are as follows (in billions): 2005 - $2.1;2006 - $1.5; 2007 - $1; thereafter - $0.4.</content>\\n[97, 549, 789, 567] Section-header <content>NOTE 11. NET INVESTMENT IN OPERATING LEASES – FINANCIAL SERVICES SECTOR</content>\\n[97, 572, 608, 587] Text <content>The net investment in operating leases at December 31 was as follows (in millions):</content>\\n[96, 592, 886, 675] Table <content>Table</content>\\n[97, 687, 886, 742] Text <content>Included in net investment in operating leases at December 31, 2004 were interests in operating leases and the related vehicles of about $2.5 billion that have been transferred for legal purposes to consolidated securitization SPEs and are available only for repayment of debt issued by those entities, and to pay other securitization investors and other participants; they are not available to pay our other obligations or the claims of our other creditors.</content>\\n[97, 749, 782, 776] Text <content>Minimum rentals on operating leases are contractually due as follows:2005 - $4.7 billion; 2006 - $2.2 billion;2007 - $1.3 billion; 2008 - $548 million; 2009 - $135 million; thereafter - $407 million. </content>\\n[97, 783, 887, 838] Text <content>Assets subject to operating leases are depreciated primarily on the straight-line method over the term of the lease to reduce the asset to its estimated residual value. Estimated residual values are based on assumptions for used vehicle prices at lease termination and the number of vehicles that are expected to be returned. Operating lease depreciation expense (which includes gains and losses on disposal of assets) was $6.4 billion in 2004, $8.5 billion in 2003, and $9.9 billion in 2002.</content>\\n[924, 963, 947, 979] Page-footer <content>71</content>\\n'}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(conversations, open(\"/home/t-jiaweiwang/Project/GOT-OCR2.0/datasets/DocLayNet/got_annotations/train_got_conversations_wo_others_pod_and_ocr.json\", \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "got",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
